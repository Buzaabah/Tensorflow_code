{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tOoyQ70H00_s"
   },
   "source": [
    "## MNIST\n",
    "\n",
    "Write an MNIST classifier that trains to 99% accuracy or above, and does it without a fixed number of epochs -- i.e. you should stop training once you reach that level of accuracy.\n",
    "\n",
    "Some notes:\n",
    "1. It should succeed in less than 10 epochs, so it is okay to change epochs= to 10, but nothing larger\n",
    "2. When it reaches 99% or greater it should print out the string \"Reached 99% accuracy so cancelling training!\"\n",
    "3. If you add any additional variables, make sure you use the same names as the ones used in the class\n",
    "\n",
    "I've started the code for you below -- how would you finish it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.0.0-beta0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c7/ae/cb34a1b15b9e54b9dd2959f1744860f476832eabc6da6070085245792afe/tensorflow-2.0.0b0-cp36-cp36m-macosx_10_11_x86_64.whl (87.9MB)\n",
      "\u001b[K     |████████████████████████████████| 87.9MB 9.5MB/s eta 0:00:012     |████████████████▎               | 44.6MB 11.5MB/s eta 0:00:04     |███████████████████████▏        | 63.7MB 11.4MB/s eta 0:00:03     |██████████████████████████████▌ | 83.8MB 6.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (3.6.1)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.11.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (0.1.7)\n",
      "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
      "\u001b[K     |████████████████████████████████| 501kB 8.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.0.9)\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.17.3)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.11.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (0.30.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (0.6.2)\n",
      "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1MB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (1.0.7)\n",
      "Collecting wrapt>=1.11.1\n",
      "  Downloading https://files.pythonhosted.org/packages/23/84/323c2415280bc4fc880ac5050dddfb3c8062c2552b34c2e512eb4aa68f79/wrapt-1.11.2.tar.gz\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (0.8.1)\n",
      "Requirement already satisfied: gast>=0.2.0 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tensorflow==2.0.0-beta0) (0.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta0) (38.4.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta0) (2.6.11)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta0) (0.14.1)\n",
      "Requirement already satisfied: h5py in /Users/buzaabahappy/anaconda3/lib/python3.6/site-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta0) (2.8.0)\n",
      "Building wheels for collected packages: wrapt\n",
      "  Building wheel for wrapt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.11.2-cp36-cp36m-macosx_10_7_x86_64.whl size=34851 sha256=acce5784461193221d3a31c64adcdc613964ee0adff67ff50c79f56c4598148b\n",
      "  Stored in directory: /Users/buzaabahappy/Library/Caches/pip/wheels/d7/de/2e/efa132238792efb6459a96e85916ef8597fcb3d2ae51590dfd\n",
      "Successfully built wrapt\n",
      "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tb-nightly 1.14.0a20190603 has requirement setuptools>=41.0.0, but you'll have setuptools 38.4.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tf-estimator-nightly, tb-nightly, wrapt, tensorflow\n",
      "  Found existing installation: tf-estimator-nightly 1.14.0.dev2019030115\n",
      "    Uninstalling tf-estimator-nightly-1.14.0.dev2019030115:\n",
      "      Successfully uninstalled tf-estimator-nightly-1.14.0.dev2019030115\n",
      "  Found existing installation: tb-nightly 1.14.0a20190301\n",
      "    Uninstalling tb-nightly-1.14.0a20190301:\n",
      "      Successfully uninstalled tb-nightly-1.14.0a20190301\n",
      "  Found existing installation: wrapt 1.10.11\n",
      "\u001b[31mERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.0.0-beta0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.2020 - accuracy: 0.9404\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0818 - accuracy: 0.9753\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0530 - accuracy: 0.9828\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0383 - accuracy: 0.9874\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0269 - accuracy: 0.9913\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0211 - accuracy: 0.9931\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0160 - accuracy: 0.9946\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0154 - accuracy: 0.9949\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0101 - accuracy: 0.9966\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0124 - accuracy: 0.9959\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0080 - accuracy: 0.9975\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0095 - accuracy: 0.9969s - loss: 0.0094 - \n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0080 - accuracy: 0.9972\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0064 - accuracy: 0.9977\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0076 - accuracy: 0.9973\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 4s 66us/sample - loss: 0.0084 - accuracy: 0.9974\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0057 - accuracy: 0.9981\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 6s 99us/sample - loss: 0.0072 - accuracy: 0.9978\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 5s 77us/sample - loss: 0.0047 - accuracy: 0.9983\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 4s 65us/sample - loss: 0.0065 - accuracy: 0.9980\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0064 - accuracy: 0.9981\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0052 - accuracy: 0.9982\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0048 - accuracy: 0.9986s - loss: 0.0049 - accu\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0060 - accuracy: 0.9980\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0042 - accuracy: 0.9986\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0040 - accuracy: 0.9986\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0065 - accuracy: 0.9981\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0036 - accuracy: 0.9988\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0057 - accuracy: 0.9984\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0057 - accuracy: 0.9984\n"
     ]
    }
   ],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('accuracy')<0.1):\n",
    "                print(\"\\nReached 99% accuracy so cancelling training\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "\n",
    "callbacks = myCallback()\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train/255.0\n",
    "x_test = x_test/255.0\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "   \n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs = 30, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rvXQGAA0ssC"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_mnist\n",
    "def train_mnist():\n",
    "    # Please write your code only where you are indicated.\n",
    "    # please do not remove # model fitting inline comments.\n",
    "\n",
    "    # YOUR CODE SHOULD START HERE\n",
    "    \n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('accuracy') > 0.99):\n",
    "                print(\"\\nReached 99% accuracy so cancelling training\")\n",
    "                self.model.stop_training = True\n",
    "                \n",
    "                \n",
    "    callbacks = myCallback()\n",
    "  \n",
    "    # YOUR CODE SHOULD END HERE\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "    # YOUR CODE SHOULD START HERE\n",
    "    \n",
    "    x_train = x_train/255.0\n",
    "    x_test = x_test/255.0\n",
    "\n",
    "    # YOUR CODE SHOULD END HERE\n",
    "    model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(512, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n",
    "   \n",
    "    \n",
    "        # YOUR CODE SHOULD END HERE\n",
    "  \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    # model fitting\n",
    "    history = model.fit(x_train, y_train, epochs = 30, callbacks=[callbacks])\n",
    "              # YOUR CODE SHOULD END HERE\n",
    "    \n",
    "    # model fitting\n",
    "    return history.epoch, history.history['accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9rvXQGAA0ssC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 4s 63us/sample - loss: 0.1980 - accuracy: 0.9417\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0801 - accuracy: 0.9755\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0529 - accuracy: 0.9833\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0378 - accuracy: 0.9876\n",
      "Epoch 5/30\n",
      "59296/60000 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9910\n",
      "Reached 99% accuracy so cancelling training\n",
      "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0283 - accuracy: 0.9910\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4], 0.9909833)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "introduction-tensorflow",
   "graded_item_id": "d6dew",
   "launcher_item_id": "FExZ4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
